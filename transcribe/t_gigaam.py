#!/usr/bin/env python3# -*- coding: utf-8 -*-import osos.environ["HF_TOKEN"] = ""  # ← вставь свой токен, если нужноFFMPEG_BIN = r"C:\Tools\ffmpeg-8.0.1-full_build-shared\bin"if hasattr(os, 'add_dll_directory'):    if os.path.isdir(FFMPEG_BIN):        os.add_dll_directory(FFMPEG_BIN)        print(f"FFmpeg DLL path added: {FFMPEG_BIN}")    else:        print(f"Ошибка: папка {FFMPEG_BIN} не найдена!")else:    os.environ["PATH"] = FFMPEG_BIN + os.pathsep + os.environ.get("PATH", "")import warningswarnings.filterwarnings("ignore", category=UserWarning, module="pyannote.audio")import argparseimport sysimport tempfileimport subprocessimport reimport loggingfrom pathlib import Pathfrom typing import List, Dict, Anyimport torchfrom transformers import AutoModelfrom datetime import timedeltafrom pydantic import BaseModel, Field# pyannote.audiofrom pyannote.audio import Pipeline# ============================================================================# Настройка логирования# ============================================================================logging.basicConfig(    level=logging.INFO,    format='%(asctime)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# ============================================================================# Конфиг# ============================================================================MIN_PAUSE_SEC = 60  # пауза для нового абзацаMAX_BLOCK_DURATION_SEC = 300  # макс длительность блока одного спикераCHUNK_SEC = 25.0  # длина чанка (если используем)OVERLAP_SEC = 1.0MIN_SEGMENT_DURATION_SEC = 0.8  # минимальная длительность сегмента спикера# Список прикольных бесполых имен для спикеровFUNNY_NAMES = [    "Пикачу", "Бублик", "Котлета", "Зефирка", "Кактус",    "Вафля", "Печенька", "Шарик", "Кнопка", "Носок",    "Байт", "Пиксель", "Глюк", "Фикс", "Баг",    "Сэндвич", "Маффин", "Тостер", "Пончик", "Круассан"]FUNNY_NAMES = [name.upper() for name in FUNNY_NAMES]# ============================================================================# Pydantic модели# ============================================================================class ChunkInfo(BaseModel):    start_sec: float    file_path: Pathclass SentenceWithTimestamp(BaseModel):    text: str    start: float    end: float    speaker: str = "UNKNOWN"class TextBlock(BaseModel):    start_sec: float    speaker: str    text: str# ============================================================================# Утилиты времени# ============================================================================def seconds_to_hhmmss(total_sec: float) -> str:    td = timedelta(seconds=int(total_sec))    return f"[{str(td).zfill(8)}]"def create_speaker_name_mapping(speaker_ids: List[str]) -> Dict[str, str]:    """    Создает маппинг от SPEAKER_XX к прикольным именам.    Args:        speaker_ids: Список оригинальных ID спикеров (например, ["SPEAKER_00", "SPEAKER_01"])    Returns:        Словарь с маппингом {original_id: funny_name}    """    import random    # Создаём детерминированный рандом на основе speaker_ids для стабильности    seed = hash("".join(sorted(speaker_ids)))    rng = random.Random(seed)    # Перемешиваем имена    shuffled_names = FUNNY_NAMES.copy()    rng.shuffle(shuffled_names)    # Создаём маппинг    mapping = {}    for idx, speaker_id in enumerate(sorted(speaker_ids)):        if idx < len(shuffled_names):            mapping[speaker_id] = f"{idx + 1:d}.{shuffled_names[idx]}"        else:            # Если спикеров больше чем имён - добавляем номер            mapping[speaker_id] = f"{idx + 1:d}.{shuffled_names[idx % len(shuffled_names)]}-{idx // len(shuffled_names) + 1}"    logger.info(        f"Создан маппинг имён спикеров:\n" +        "\n".join(f"  {k} → {v}" for k, v in mapping.items())    )    return mappingdef is_video_file(file_path: Path) -> bool:    """Проверяет, является ли файл видео"""    video_extensions = {'.mp4', '.avi', '.mkv', '.mov', '.wmv', '.flv', '.webm', '.m4v', '.mpg', '.mpeg'}    return file_path.suffix.lower() in video_extensionsdef needs_audio_conversion(file_path: Path) -> bool:    """    Проверяет, нужна ли конвертация аудио для pyannote.    AMR, M4A и некоторые другие форматы вызывают проблемы с сэмплами.    """    problematic_formats = {'.amr', '.m4a', '.aac', '.3gp', '.opus'}    return file_path.suffix.lower() in problematic_formatsdef convert_audio_to_wav(audio_path: Path, output_dir: Path) -> Path:    """    Конвертирует аудиофайл в WAV формат для корректной работы pyannote.    Args:        audio_path: Путь к исходному аудиофайлу        output_dir: Директория для сохранения WAV    Returns:        Путь к сконвертированному WAV файлу    """    wav_path = output_dir / f"{audio_path.stem}_converted.wav"    logger.info(f"Конвертация аудио в WAV для pyannote: {audio_path.name}")    cmd = [        "ffmpeg", "-y", "-i", str(audio_path),        "-acodec", "pcm_s16le",  # PCM 16-bit        "-ar", "16000",  # Sample rate 16kHz        "-ac", "1",  # Mono        str(wav_path)    ]    logger.info(f"Команда: {' '.join(cmd)}")    result = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", errors="replace")    if result.returncode != 0:        logger.error(f"Ошибка конвертации аудио: {result.stderr}")        raise RuntimeError(f"FFmpeg failed to convert audio from {audio_path}")    logger.info(f"✓ Аудио сконвертировано: {wav_path.name} ({wav_path.stat().st_size / 1024 / 1024:.2f} MB)")    return wav_pathdef extract_audio_from_video(video_path: Path, output_dir: Path) -> Path:    """    Извлекает аудио из видеофайла в WAV формат для обработки.    Args:        video_path: Путь к видеофайлу        output_dir: Директория для сохранения аудио    Returns:        Путь к извлечённому аудиофайлу    """    audio_path = output_dir / f"{video_path.stem}_audio.wav"    logger.info(f"Извлечение аудио из видео: {video_path.name}")    cmd = [        "ffmpeg", "-y", "-i", str(video_path),        "-vn",  # Без видео        "-acodec", "pcm_s16le",  # PCM 16-bit        "-ar", "16000",  # Sample rate 16kHz        "-ac", "1",  # Mono        str(audio_path)    ]    logger.info(f"Команда: {' '.join(cmd)}")    result = subprocess.run(cmd, capture_output=True, text=True, encoding="utf-8", errors="replace")    if result.returncode != 0:        logger.error(f"Ошибка извлечения аудио: {result.stderr}")        raise RuntimeError(f"FFmpeg failed to extract audio from {video_path}")    logger.info(f"✓ Аудио извлечено: {audio_path.name} ({audio_path.stat().st_size / 1024 / 1024:.2f} MB)")    return audio_pathdef get_audio_duration(input_path: Path) -> float:    cmd = ["ffmpeg", "-i", str(input_path), "-f", "null", "-"]    logger.debug(f"Команда получения длительности: {' '.join(cmd)}")    result = subprocess.run(cmd, stderr=subprocess.PIPE, text=True, encoding="utf-8", errors="replace")    for line in result.stderr.splitlines():        if "Duration:" in line:            dur_str = line.split("Duration:")[1].split(",")[0].strip()            h, m, s_ms = dur_str.split(":")            s, _ = s_ms.split(".")            return int(h) * 3600 + int(m) * 60 + int(s)    return 0.0# ============================================================================# Диаризация pyannote# ============================================================================def diarize_audio(file_path: str, device: str) -> List[Dict[str, Any]]:    """Возвращает список сегментов спикеров"""    logger.info("→ Запуск pyannote speaker diarization...")    logger.info(f"  Устройство для диаризации: {device.upper()}")    # Allowlist для PyTorch 2.6+ (если нужно — оставляем)    import torch.torch_version    from pyannote.audio.core.task import Specifications, Problem, Resolution    from omegaconf import ListConfig    from pyannote.audio.core.model import Introspection    torch.serialization.add_safe_globals([        torch.torch_version.TorchVersion,        Specifications,        Problem,        Resolution,        ListConfig,        Introspection,    ])    pipeline = Pipeline.from_pretrained(        "pyannote/speaker-diarization-3.1",        token=os.environ.get("HF_TOKEN")    )    pipeline = pipeline.to(torch.device(device))    logger.info(f"✓ Pyannote pipeline загружен на {device.upper()}")    # Запускаем диаризацию (это может занять время)    logger.info(f"→ Запуск анализа спикеров (это может занять несколько минут)...")    logger.info(f"  Файл: {Path(file_path).name}")    logger.info(f"  Длительность: {get_audio_duration(Path(file_path)):.1f} сек")    # num_speakers=2 — сильно помогает на диалогах врач+пациент    diarization = pipeline(file_path, num_speakers=2)    logger.info(f"✓ Диаризация завершена")    annotation = diarization.speaker_diarization    segments = []    for turn, _, speaker in annotation.itertracks(yield_label=True):        duration = turn.end - turn.start        if duration >= MIN_SEGMENT_DURATION_SEC:            segments.append({                "start": turn.start,                "end": turn.end,                "speaker": speaker,                "duration": duration            })    logger.info(f"Найдено {len(segments)} сегментов спикеров после фильтрации")    return segmentsdef extract_chunk(file_path: Path, start: float, end: float, tmp_dir: Path) -> Path:    chunk_file = tmp_dir / f"chunk_{start:.1f}_{end:.1f}.wav"    cmd = [        "ffmpeg", "-y", "-i", str(file_path),        "-ss", str(start),        "-t", str(end - start),        "-ar", "16000", "-ac", "1", "-c:a", "pcm_s16le",        str(chunk_file)    ]    logger.debug(f"Команда извлечения чанка: {' '.join(cmd)}")    subprocess.run(cmd, check=True, capture_output=True)    return chunk_file# ============================================================================# Группировка по спикерам# ============================================================================def group_by_speaker(blocks: List[TextBlock]) -> List[TextBlock]:    """    Группирует последовательные блоки одного спикера в один сегмент.    Логика:    - Если следующий блок от того же спикера - объединяем тексты    - Если спикер переключается - создаем новый блок    - Учитываем паузы: если пауза > MIN_PAUSE_SEC, создаем новый блок даже для того же спикера    """    if not blocks:        return []    # Сортируем по времени начала    sorted_blocks = sorted(blocks, key=lambda b: b.start_sec)    grouped = []    current_speaker = None    current_texts = []    current_start = None    last_end = 0.0    for block in sorted_blocks:        pause = block.start_sec - last_end        # Условия для создания нового блока:        # 1. Сменился спикер        # 2. Большая пауза (даже у того же спикера)        should_create_new = (                current_speaker is not None and                (block.speaker != current_speaker or pause >= MIN_PAUSE_SEC)        )        if should_create_new:            # Сохраняем накопленный блок            combined_text = " ".join(current_texts)            grouped.append(TextBlock(                start_sec=current_start,                speaker=current_speaker,                text=combined_text            ))            logger.debug(                f"Создан блок: [{seconds_to_hhmmss(current_start)}] {current_speaker}, "                f"предложений: {len(current_texts)}, символов: {len(combined_text)}"            )            current_texts = []            current_speaker = None            current_start = None        # Начинаем новый блок или продолжаем текущий        if current_speaker is None:            current_speaker = block.speaker            current_start = block.start_sec        current_texts.append(block.text)        # Приблизительная оценка длительности блока        last_end = block.start_sec + (len(block.text.split()) / 150 * 60)    # Не забываем последний блок    if current_texts:        combined_text = " ".join(current_texts)        grouped.append(TextBlock(            start_sec=current_start,            speaker=current_speaker,            text=combined_text        ))        logger.debug(            f"Создан блок: [{seconds_to_hhmmss(current_start)}] {current_speaker}, "            f"предложений: {len(current_texts)}, символов: {len(combined_text)}"        )    return grouped# ============================================================================# Основная логика транскрипции по спикерам# ============================================================================def transcribe_with_speakers(        file_path: str,        revision: str = "e2e_rnnt",        device: str = "cuda" if torch.cuda.is_available() else "cpu",):    input_path = Path(file_path)    logger.info(f"Начало обработки: {input_path.name}")    # Создаём временную директорию    tmp_dir = Path(tempfile.mkdtemp(prefix="gigaam_diar_"))    converted_audio_path = None  # Для очистки    # Проверяем тип файла и конвертируем если нужно    is_video = is_video_file(input_path)    needs_conversion = needs_audio_conversion(input_path)    if is_video:        logger.info(f"Обнаружен видеофайл: {input_path.suffix}")        # Извлекаем аудио из видео        converted_audio_path = extract_audio_from_video(input_path, tmp_dir)        process_path = str(converted_audio_path)    elif needs_conversion:        logger.info(f"Обнаружен аудиофайл с проблемным форматом: {input_path.suffix}")        # Конвертируем в WAV для pyannote        converted_audio_path = convert_audio_to_wav(input_path, tmp_dir)        process_path = str(converted_audio_path)    else:        logger.info(f"Обнаружен аудиофайл: {input_path.suffix}")        process_path = file_path    logger.info(f"→ Загрузка модели GigaAM-v3 ({revision}) на устройство: {device.upper()}...")    model = AutoModel.from_pretrained(        "ai-sage/GigaAM-v3",        revision=revision,        trust_remote_code=True,    )    model.to(device)    model.eval()    logger.info(f"✓ Модель загружена на {device.upper()}")    # 1. Диаризация (используем сконвертированное аудио или оригинальный файл)    speaker_segments = diarize_audio(process_path, device)    # Создаём маппинг спикеров на прикольные имена    unique_speakers = sorted(set(seg["speaker"] for seg in speaker_segments))    speaker_name_mapping = create_speaker_name_mapping(unique_speakers)    # 2. Транскрипция каждого сегмента спикера    all_blocks: List[TextBlock] = []    for seg in speaker_segments:        start, end, original_speaker = seg["start"], seg["end"], seg["speaker"]        funny_name = speaker_name_mapping[original_speaker]        duration = end - start        if duration < 1.0:            continue        logger.info(f"Транскрипция сегмента [{start:.1f}s → {end:.1f}s] {funny_name} ({duration:.1f}с)")        chunk_path = extract_chunk(Path(process_path), start, end, tmp_dir)        try:            with torch.inference_mode():                text = model.transcribe(str(chunk_path)).strip()            if not text:                continue            # Разбиваем на предложения            sentences = re.split(r'(?<=[.!?])\s+', text.strip())            sentences = [s.strip() for s in sentences if s.strip()]            # Добавляем маркер [...] только если есть предложения            if sentences:                sentences[-1] = sentences[-1] + " [...]"            # Собираем в блок с прикольным именем            block_text = " ".join(sentences)            # Добавляем блок только если есть текст            if block_text:                block = TextBlock(                    start_sec=start,                    speaker=funny_name,                    text=block_text                )                all_blocks.append(block)                logger.info(f"  {funny_name}: {block_text[:80]}...")            else:                logger.debug(f"  Пропущен пустой сегмент для {funny_name}")        except Exception as e:            logger.error(f"Ошибка на сегменте {start:.1f}–{end:.1f}: {e}")        finally:            if chunk_path.exists():                chunk_path.unlink()    # Удаляем временные файлы    if converted_audio_path and converted_audio_path.exists():        logger.debug(f"Удаляем сконвертированное аудио: {converted_audio_path.name}")        converted_audio_path.unlink()    tmp_dir.rmdir() if tmp_dir.exists() else None    # 3. Группировка последовательных сегментов одного спикера    logger.info(f"Группировка {len(all_blocks)} блоков по спикерам...")    grouped_blocks = group_by_speaker(all_blocks)    logger.info(f"Получено {len(grouped_blocks)} сгруппированных блоков")    # 4. Формирование финального текста    output_lines = []    for group in grouped_blocks:        timestamp = seconds_to_hhmmss(group.start_sec)        output_lines.append(f"{timestamp} {group.speaker}: {group.text}")    full_text = "\n\n".join(output_lines)    # Сохранение (используем стандартный паттерн имени: <имя>.gigaam-<revision>-speakers.txt)    out_path = input_path.parent / f"{input_path.stem}.gigaam-{revision}-speakers.txt"    out_path.write_text(full_text, encoding="utf-8")    # Копируем время модификации исходного файла    original_mtime = input_path.stat().st_mtime    os.utime(out_path, (original_mtime, original_mtime))    logger.info(f"Готово! Результат: {out_path}")def main():    parser = argparse.ArgumentParser(description="GigaAM-v3 + pyannote speakers")    parser.add_argument("input", nargs="+", help="файл или папка")    parser.add_argument("--revision", default="e2e_rnnt", choices=["e2e_rnnt", "e2e_ctc", "rnnt", "ctc"])    parser.add_argument("--device", default="auto", choices=["auto", "cpu", "cuda"])    args = parser.parse_args()    # Определение устройства с детальным логированием    cuda_available = torch.cuda.is_available()    if args.device == "auto":        device = "cuda" if cuda_available else "cpu"    else:        device = args.device    # Логируем информацию о GPU    logger.info(        f"{'=' * 80}\n"        f"CUDA доступна: {cuda_available}\n"        f"Выбранное устройство: {device.upper()}\n"        f"{'=' * 80}"    )    if cuda_available and device == "cuda":        gpu_name = torch.cuda.get_device_name(0)        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024 ** 3        logger.info(            f"GPU информация:\n"            f"  Название: {gpu_name}\n"            f"  Память: {gpu_memory:.2f} GB\n"            f"  Текущий индекс: {torch.cuda.current_device()}"        )    elif device == "cuda" and not cuda_available:        logger.warning("⚠️  CUDA запрошена, но не доступна! Используется CPU.")        device = "cpu"    paths = []    for p in args.input:        path = Path(p).expanduser().resolve()        if path.is_dir():            # Ищем аудио файлы            for ext in ("*.wav", "*.mp3", "*.m4a", "*.ogg", "*.flac"):                paths.extend(path.rglob(ext))            # Ищем видео файлы            for ext in ("*.mp4", "*.avi", "*.mkv", "*.mov", "*.wmv", "*.flv", "*.webm", "*.m4v", "*.mpg", "*.mpeg"):                paths.extend(path.rglob(ext))        elif path.is_file():            paths.append(path)    if not paths:        logger.error("Не найдено аудио/видео файлов")        sys.exit(1)    for path in sorted(paths):        print(f"\n{'═' * 80}")        print(f"Обрабатываем: {path}")        try:            transcribe_with_speakers(                str(path),                revision=args.revision,                device=device,            )        except Exception as e:            print(f"Ошибка: {type(e).__name__} → {e}", file=sys.stderr)if __name__ == "__main__":    sys.exit(main() or 0)