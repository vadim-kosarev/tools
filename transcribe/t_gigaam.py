#!/usr/bin/env python3# -*- coding: utf-8 -*-import osos.environ["HF_TOKEN"] = ""  # ← вставь свой токен, если нужноFFMPEG_BIN = r"C:\Tools\ffmpeg-8.0.1-full_build-shared\bin"if hasattr(os, 'add_dll_directory'):    if os.path.isdir(FFMPEG_BIN):        os.add_dll_directory(FFMPEG_BIN)        print(f"FFmpeg DLL path added: {FFMPEG_BIN}")    else:        print(f"Ошибка: папка {FFMPEG_BIN} не найдена!")else:    os.environ["PATH"] = FFMPEG_BIN + os.pathsep + os.environ.get("PATH", "")import warningswarnings.filterwarnings("ignore", category=UserWarning, module="pyannote.audio")import argparseimport sysimport tempfileimport subprocessimport reimport loggingfrom pathlib import Pathfrom typing import List, Dict, Anyimport torchfrom transformers import AutoModelfrom datetime import timedeltafrom pydantic import BaseModel, Field# pyannote.audiofrom pyannote.audio import Pipeline# ============================================================================# Настройка логирования# ============================================================================logging.basicConfig(    level=logging.INFO,    format='%(asctime)s - %(levelname)s - %(message)s')logger = logging.getLogger(__name__)# ============================================================================# Конфиг# ============================================================================MIN_PAUSE_SEC = 60              # пауза для нового абзацаMAX_BLOCK_DURATION_SEC = 300    # макс длительность блока одного спикераCHUNK_SEC = 25.0                # длина чанка (если используем)OVERLAP_SEC = 1.0MIN_SEGMENT_DURATION_SEC = 0.8  # минимальная длительность сегмента спикера# Список прикольных бесполых имен для спикеровFUNNY_NAMES = [    "Пикачу", "Бублик", "Котлета", "Зефирка", "Кактус",    "Вафля", "Печенька", "Шарик", "Кнопка", "Носок",    "Байт", "Пиксель", "Глюк", "Фикс", "Баг",    "Сэндвич", "Маффин", "Тостер", "Пончик", "Круассан"]FUNNY_NAMES = [name.upper() for name in FUNNY_NAMES]# ============================================================================# Pydantic модели# ============================================================================class ChunkInfo(BaseModel):    start_sec: float    file_path: Pathclass SentenceWithTimestamp(BaseModel):    text: str    start: float    end: float    speaker: str = "UNKNOWN"class TextBlock(BaseModel):    start_sec: float    speaker: str    text: str# ============================================================================# Утилиты времени# ============================================================================def seconds_to_hhmmss(total_sec: float) -> str:    td = timedelta(seconds=int(total_sec))    return f"[{str(td).zfill(8)}]"def create_speaker_name_mapping(speaker_ids: List[str]) -> Dict[str, str]:    """    Создает маппинг от SPEAKER_XX к прикольным именам.    Args:        speaker_ids: Список оригинальных ID спикеров (например, ["SPEAKER_00", "SPEAKER_01"])    Returns:        Словарь с маппингом {original_id: funny_name}    """    import random    # Создаём детерминированный рандом на основе speaker_ids для стабильности    seed = hash("".join(sorted(speaker_ids)))    rng = random.Random(seed)    # Перемешиваем имена    shuffled_names = FUNNY_NAMES.copy()    rng.shuffle(shuffled_names)    # Создаём маппинг    mapping = {}    for idx, speaker_id in enumerate(sorted(speaker_ids)):        if idx < len(shuffled_names):            mapping[speaker_id] = shuffled_names[idx]        else:            # Если спикеров больше чем имён - добавляем номер            mapping[speaker_id] = f"{shuffled_names[idx % len(shuffled_names)]}-{idx // len(shuffled_names) + 1}"    logger.info(        f"Создан маппинг имён спикеров:\n" +        "\n".join(f"  {k} → {v}" for k, v in mapping.items())    )    return mappingdef get_audio_duration(input_path: Path) -> float:    cmd = ["ffmpeg", "-i", str(input_path), "-f", "null", "-"]    result = subprocess.run(cmd, stderr=subprocess.PIPE, text=True, encoding="utf-8", errors="replace")    for line in result.stderr.splitlines():        if "Duration:" in line:            dur_str = line.split("Duration:")[1].split(",")[0].strip()            h, m, s_ms = dur_str.split(":")            s, _ = s_ms.split(".")            return int(h)*3600 + int(m)*60 + int(s)    return 0.0# ============================================================================# Диаризация pyannote# ============================================================================def diarize_audio(file_path: str, device: str) -> List[Dict[str, Any]]:    """Возвращает список сегментов спикеров"""    logger.info("→ Запуск pyannote speaker diarization...")    logger.info(f"  Устройство для диаризации: {device.upper()}")    # Allowlist для PyTorch 2.6+ (если нужно — оставляем)    import torch.torch_version    from pyannote.audio.core.task import Specifications, Problem, Resolution    from omegaconf import ListConfig    from pyannote.audio.core.model import Introspection    torch.serialization.add_safe_globals([        torch.torch_version.TorchVersion,        Specifications,        Problem,        Resolution,        ListConfig,        Introspection,    ])    pipeline = Pipeline.from_pretrained(        "pyannote/speaker-diarization-3.1",        token=os.environ.get("HF_TOKEN")    )    pipeline.to(torch.device(device))    logger.info(f"✓ Pyannote pipeline загружен на {device.upper()}")    # num_speakers=2 — сильно помогает на диалогах врач+пациент    diarization = pipeline(file_path, num_speakers=2)    annotation = diarization.speaker_diarization    segments = []    for turn, _, speaker in annotation.itertracks(yield_label=True):        duration = turn.end - turn.start        if duration >= MIN_SEGMENT_DURATION_SEC:            segments.append({                "start": turn.start,                "end": turn.end,                "speaker": speaker,                "duration": duration            })    logger.info(f"Найдено {len(segments)} сегментов спикеров после фильтрации")    return segmentsdef extract_chunk(file_path: Path, start: float, end: float, tmp_dir: Path) -> Path:    chunk_file = tmp_dir / f"chunk_{start:.1f}_{end:.1f}.wav"    cmd = [        "ffmpeg", "-y", "-i", str(file_path),        "-ss", str(start),        "-t", str(end - start),        "-ar", "16000", "-ac", "1", "-c:a", "pcm_s16le",        str(chunk_file)    ]    subprocess.run(cmd, check=True, capture_output=True)    return chunk_file# ============================================================================# Группировка по спикерам# ============================================================================def group_by_speaker(blocks: List[TextBlock]) -> List[TextBlock]:    """    Группирует последовательные блоки одного спикера в один сегмент.    Логика:    - Если следующий блок от того же спикера - объединяем тексты    - Если спикер переключается - создаем новый блок    - Учитываем паузы: если пауза > MIN_PAUSE_SEC, создаем новый блок даже для того же спикера    """    if not blocks:        return []    # Сортируем по времени начала    sorted_blocks = sorted(blocks, key=lambda b: b.start_sec)    grouped = []    current_speaker = None    current_texts = []    current_start = None    last_end = 0.0    for block in sorted_blocks:        pause = block.start_sec - last_end        # Условия для создания нового блока:        # 1. Сменился спикер        # 2. Большая пауза (даже у того же спикера)        should_create_new = (            current_speaker is not None and            (block.speaker != current_speaker or pause >= MIN_PAUSE_SEC)        )        if should_create_new:            # Сохраняем накопленный блок            combined_text = " ".join(current_texts)            grouped.append(TextBlock(                start_sec=current_start,                speaker=current_speaker,                text=combined_text            ))            logger.debug(                f"Создан блок: [{seconds_to_hhmmss(current_start)}] {current_speaker}, "                f"предложений: {len(current_texts)}, символов: {len(combined_text)}"            )            current_texts = []            current_speaker = None            current_start = None        # Начинаем новый блок или продолжаем текущий        if current_speaker is None:            current_speaker = block.speaker            current_start = block.start_sec        current_texts.append(block.text)        # Приблизительная оценка длительности блока        last_end = block.start_sec + (len(block.text.split()) / 150 * 60)    # Не забываем последний блок    if current_texts:        combined_text = " ".join(current_texts)        grouped.append(TextBlock(            start_sec=current_start,            speaker=current_speaker,            text=combined_text        ))        logger.debug(            f"Создан блок: [{seconds_to_hhmmss(current_start)}] {current_speaker}, "            f"предложений: {len(current_texts)}, символов: {len(combined_text)}"        )    return grouped# ============================================================================# Основная логика транскрипции по спикерам# ============================================================================def transcribe_with_speakers(        file_path: str,        revision: str = "e2e_rnnt",        device: str = "cuda" if torch.cuda.is_available() else "cpu",):    logger.info(f"Начало обработки: {file_path}")    logger.info(f"→ Загрузка модели GigaAM-v3 ({revision}) на устройство: {device.upper()}...")    model = AutoModel.from_pretrained(        "ai-sage/GigaAM-v3",        revision=revision,        trust_remote_code=True,    )    model.to(device)    model.eval()    logger.info(f"✓ Модель загружена на {device.upper()}")    tmp_dir = Path(tempfile.mkdtemp(prefix="gigaam_diar_"))    # 1. Диаризация    speaker_segments = diarize_audio(file_path, device)    # Создаём маппинг спикеров на прикольные имена    unique_speakers = sorted(set(seg["speaker"] for seg in speaker_segments))    speaker_name_mapping = create_speaker_name_mapping(unique_speakers)    # 2. Транскрипция каждого сегмента спикера    all_blocks: List[TextBlock] = []    for seg in speaker_segments:        start, end, original_speaker = seg["start"], seg["end"], seg["speaker"]        funny_name = speaker_name_mapping[original_speaker]        duration = end - start        if duration < 1.0:            continue        logger.info(f"Транскрипция сегмента [{start:.1f}s → {end:.1f}s] {funny_name} ({duration:.1f}с)")        chunk_path = extract_chunk(Path(file_path), start, end, tmp_dir)        try:            with torch.inference_mode():                text = model.transcribe(str(chunk_path)).strip()            if not text:                continue            # Разбиваем на предложения            sentences = re.split(r'(?<=[.!?])\s+', text.strip())            sentences = [s.strip() for s in sentences if s.strip()]            # Собираем в блок с прикольным именем            block_text = " ".join(sentences)            block = TextBlock(                start_sec=start,                speaker=funny_name,                text=block_text            )            all_blocks.append(block)            logger.info(f"  {funny_name}: {block_text[:80]}...")        except Exception as e:            logger.error(f"Ошибка на сегменте {start:.1f}–{end:.1f}: {e}")        finally:            if chunk_path.exists():                chunk_path.unlink()    tmp_dir.rmdir() if tmp_dir.exists() else None    # 3. Группировка последовательных сегментов одного спикера    logger.info(f"Группировка {len(all_blocks)} блоков по спикерам...")    grouped_blocks = group_by_speaker(all_blocks)    logger.info(f"Получено {len(grouped_blocks)} сгруппированных блоков")    # 4. Формирование финального текста    output_lines = []    for group in grouped_blocks:        timestamp = seconds_to_hhmmss(group.start_sec)        output_lines.append(f"{timestamp} {group.speaker}: {group.text}")    full_text = "\n\n".join(output_lines)    # Сохранение    out_path = Path(file_path).with_suffix(f".gigaam-{revision}-speakers.txt")    out_path.write_text(full_text, encoding="utf-8")    logger.info(f"Готово! Результат: {out_path}")def main():    parser = argparse.ArgumentParser(description="GigaAM-v3 + pyannote speakers")    parser.add_argument("input", nargs="+", help="файл или папка")    parser.add_argument("--revision", default="e2e_rnnt", choices=["e2e_rnnt", "e2e_ctc", "rnnt", "ctc"])    parser.add_argument("--device", default="auto", choices=["auto", "cpu", "cuda"])    args = parser.parse_args()    # Определение устройства с детальным логированием    cuda_available = torch.cuda.is_available()    if args.device == "auto":        device = "cuda" if cuda_available else "cpu"    else:        device = args.device    # Логируем информацию о GPU    logger.info(        f"{'=' * 80}\n"        f"CUDA доступна: {cuda_available}\n"        f"Выбранное устройство: {device.upper()}\n"        f"{'=' * 80}"    )    if cuda_available and device == "cuda":        gpu_name = torch.cuda.get_device_name(0)        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3        logger.info(            f"GPU информация:\n"            f"  Название: {gpu_name}\n"            f"  Память: {gpu_memory:.2f} GB\n"            f"  Текущий индекс: {torch.cuda.current_device()}"        )    elif device == "cuda" and not cuda_available:        logger.warning("⚠️  CUDA запрошена, но не доступна! Используется CPU.")        device = "cpu"    paths = []    for p in args.input:        path = Path(p).expanduser().resolve()        if path.is_dir():            for ext in ("*.wav", "*.mp3", "*.m4a", "*.ogg", "*.flac"):                paths.extend(path.rglob(ext))        elif path.is_file():            paths.append(path)    if not paths:        print("Не найдено аудиофайлов", file=sys.stderr)        sys.exit(1)    for path in sorted(paths):        print(f"\n{'═' * 80}")        print(f"Обрабатываем: {path}")        try:            transcribe_with_speakers(                str(path),                revision=args.revision,                device=device,            )        except Exception as e:            print(f"Ошибка: {type(e).__name__} → {e}", file=sys.stderr)if __name__ == "__main__":    sys.exit(main() or 0)